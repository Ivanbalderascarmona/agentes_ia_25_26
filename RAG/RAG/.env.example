# ==========================
# Configuración de Ollama
# ==========================
# URL del servidor Ollama local o remoto
OLLAMA_URL=http://url:11434

# Modelo de embeddings para generación de vectores semánticos
OLLAMA_MODEL_EMBEDDINGS=modelo_ollama_embeddings

# Modelo principal de lenguaje (LLM)
OLLAMA_MODEL_LLM=modelo_llm


# ==========================
# Configuración de Qdrant
# ==========================
# URL del servidor Qdrant (debe coincidir con docker-compose)
QDRANT_URL=http://localhost:6333

# Nombre de la colección para almacenar fragmentos
QDRANT_COLLECTION_NAME=fragmentos_rof


# ==========================
# Configuración de Node.js
# ==========================
# Entorno de ejecución (development | production)
NODE_ENV=development
